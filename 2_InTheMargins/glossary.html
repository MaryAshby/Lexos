<p>This page is intended to provide definitions for the terms used within the Lexos suite, as well as to disambiguate terms drawn from natural language, programming languages, and linguistic analysis.</p>

<p><a name="agglomerative-hierarchical-clustering"></a> <strong><u>Agglomerative Hierarchical Clustering</u></strong></p>

<p><a name="character"></a> <strong><u>Character</u></strong>
<p>A character is any individual symbol. The letters that make up the Roman alphabet are characters, as are non-alphabetic symbols such as the Hanzi used in Chinese writing. In Lexos, the term <em>character</em> generally refers to countable symbols.</p>

<p><a name="community-detection"></a> <strong><u>Community Detection</u></strong></p>

<p><a name="cosine-similarity"></a> <strong><u>Cosine Similarity</u></strong></p>

<p><a name="cutting"></a> <strong><u>Cutting</u></strong></p>

<p><a name="dendrogram"></a> <strong><u>Dendrogram</u></strong></p>

<p><a name="dimensionality-reduction"></a> <strong><u>Dimensionality Reduction</u></strong></p>

<p><a name="distance-metric"></a> <strong><u>Distance Metric</u></strong></p>

<p><a name="document"></a> <strong><u>Document</u></strong>
<p>In Lexos, a document is any collection of words (known as terms in Lexos) or characters collected together to form a single item within the Lexos tool. A document is distinct from a file in that the term document refers specifically to the items manipulated within the Lexos software suite, as opposed to file, which refers to the items that are either uploaded from or downloaded to a user&rsquo;s device.</p>

<p><a name="edit-distance"></a> <strong><u>Edit Distance</u></strong></p>

<p><a name="euclidean-distance"></a> <strong><u>Euclidean Distance</u></strong></p>

<p><a name="exclusive-cluster-analysis"></a> <strong><u>Exclusive Cluster Analysis</u></strong></p>

<p><a name="feature-selection"></a> <strong><u>Feature Selection</u></strong></p>

<p><a name="file"></a> <strong><u>File</u></strong>
<p>File refers to items that can be manipulated through the file manager on a user&rsquo;s computer i.e. windows explorer, archive manager, etc. File is only used in the Lexos suite when referring to functions that involve the user&rsquo;s file system, such as uploading or downloading.</p>

<p><a name="flat-cluster-analysis"></a> <strong><u>Flat Cluster Analysis</u></strong></p>

<p><a name="hapax-legomena"></a> <strong><u><em>Lapax Legomena</em></u></strong></p>
<p>A term occurring only once in a document or corpus.</p>

<p><a name="hierarchical-cluster-analysis"></a> <strong><u>Hierarchical Cluster Analysis</u></strong></p>

<p><a name="k-means-clustering"></a> <strong><u>K-Means Clustering</u></strong></p>

<p><a name="lemma"></a> <strong><u>Lemma</u></strong>
<p>The dictionary headword form of a word. For instance, &ldquo;cat&rdquo; is the lemma for &ldquo;cat&rdquo;, &ldquo;cats&rdquo;, &ldquo;cat&rsquo;s&rdquo;, and &ldquo;cats&rsquo;&rdquo;. Lemmas are generally used to consolidate grammatical variations of the same word as a single term, but they may also be used for spelling variants.</p>

<p><a name="lexomics"></a> <strong><u>Lexomics</u></strong>
<p>The term &ldquo;lexomics&rdquo; was originally used to describe the computer-assisted detection of &ldquo;words&rdquo; (short sequences of bases) in genomes,<sup><a href="http://www.jstor.org/stable/10.1086/668252#fn15">*</a></sup> but we have extended it to apply to literature, where lexomics is the analysis of the frequency, distribution, and arrangement of words in large-scale patterns. Using statistical methods and computer-based tools to analyze data retrieved from electronic corpora, lexomic analysis allows us to identify patterns of vocabulary use that are too subtle or diffuse to be perceived easily. We then use the results derived from statistical and computer-based analysis to augment traditional literary approaches including close reading, philological analysis, and source study. Lexomics thus combines information processing and analysis with methods developed by medievalists over the past two centuries. We can use traditional methods to identify problems that can be addressed in new ways by lexomics, and we also use the results of lexomic analysis to help us zero in on textual relationships or portions of texts that might not previously have received much attention.</p>

<p><a name="n-gram"></a> <strong><u>N-gram</u></strong>
<p>An n-gram is a string of one or more tokens delimited by length. N-grams can be characters or larger tokens (e.g. space-bounded strings typically equivalent to words in Western languages). A one-character n-gram is described as a 1-gram or uni-gram. There are also 2-grams (bi-grams), 3-grams (tri-grams), 4-grams, and 5-grams. Larger n-grams are rarely used. Using n-grams to create a sliding window of characters in a text is one method of counting terms in non-Western languages (or DNA sequences) where spaces or other markers are not used to delimit token boundaries.</p>

<p><a name="normalization"></a> <strong><u>Normalization</u></strong></p>

<p><a name="overlapping-cluster-analysis"></a> <strong><u>Overlapping Cluster Analysis</u></strong></p>

<p><a name="partitioning-cluster-analysis"></a> <strong><u>Partitioning Cluster Analysis</u></strong></p>

<p><a name="rolling-window-analysis"></a> <strong><u>Rolling Window Analysis</u></strong></p>

<p><a name="scrubbing"></a> <strong><u>Scrubbing</u></strong></p>

<p><a name="segment"></a> <strong><u>Segment</u></strong>
<p>After cutting a text in Lexos, the separated pieces of the text are referred to as segments. However, segments are treated by Lexos as documents and they may be referred to as documents when the focus is not on their being a part of the entire text.</p>

<p><a name="similarity"></a> <strong><u>Similarity</u></strong></p>

<p><a name="sparse-matrix"></a> <strong><u>Sparse Matrix</u></strong></p>

<p><a name="standard-deviation"></a> <strong><u>Standard Deviation</u></strong></p>

<p><a name="standard-error-test"></a> <strong><u>Standard Error Test</u></strong></p>

<p><a name="stopword"></a> <strong><u>Stopword</u></strong></p>

<p><a name="supervised-learning"></a> <strong><u>Supervised Learning</u></strong></p>

<p><a name="term"></a> <strong><u>Term</u></strong>
<p>A term is the unique form of a token. If a <strong>token</strong> &ldquo;cat&rdquo; occurs two times in a document, the <strong>term</strong> count for &ldquo;cat&rdquo; is 2. In computational linguistics, terms are sometimes called &ldquo;types&rdquo;, but we avoid this usage for consistency.</p>

<p><a name="text"></a> <strong><u>Text</u></strong>
<p>Text is a general term used to refer to the objects studied in lexomics, irrespective of the form. It thus may refer to either a file or documents, but it is typically used to refer to the whole work, rather than smaller segments.</p>

<p><a name="token"></a> <strong><u>Token</u></strong>
<p>A token is an individual string of characters that may occur any number of times in a document. Tokens can be characters, words, or n-grams (strings of one or more characters or words).</p>

<p><a name="tokenization"></a> <strong><u>Tokenization</u></strong>
<p>The process of dividing a text into <em>tokens</em>.</p>

<p><a name="type"></a> <strong><u>Type</u></strong>
<p>See <strong>term</strong>.</p>

<p><a name="unicode"></a> <strong><u>Unicode</u></strong></p>

<p><a name="unsupervised-learning"></a> <strong><u>Unsupervised Learning</u></strong></p>

<p><a name="word"></a> <strong><u>Word</u></strong></p>
<p>A word is, in many Western languages, a set of characters bounded by whitespace or punctuation marks, where whitespace refers to one or more spaces, tabs, or new-line inserts. However, to avoid ambiguity when dealing with many non-Western languages such as Chinese, where a single Hanzi character can refer to the equivalent of an entire Western word, <em>term</em> is used throughout throughout the Lexos interface and documentation in place of <em>word</em>. There are a few exceptions where &ldquo;word&rdquo; is used because it is part of an established phrase, it is less awkward, or because the context refers to the semantic category of words.</p>